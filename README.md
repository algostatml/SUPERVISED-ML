# SUPERVISED-ML
Supervised machine learning algorithms explained with code

----------------------
##### Logistic Regression | [code](https://github.com/algostatml/SUPERVISED-ML/blob/master/CLASSIFICATION/LogisticRegression.py)

##### Here we present the solution to logistic regression using the cross-entropy loss function. This is a binary classifier for supervised learning and we solve the optimization function derived from the maximum likelihood using gradient ascent.
----------------------
##### Perceptron | Cross-Entropy [code](https://github.com/algostatml/SUPERVISED-ML/blob/master/CLASSIFICATION/Perceptron.py) | 

##### Single layer perceptron is the basics upon which Artificial Neural Networks (ANN) are built. We solved the optimization problem of the single layer perceptron using the 'Perceptron Convergence algorithm'
----------------------
##### Perceptron | Convergence algo [code](https://github.com/algostatml/SUPERVISED-ML/blob/master/CLASSIFICATION/Perceptron_stepwise.py)
----------------------

 ##### The loss function here is solved using the 'Stochastic gradient descent' algorihm. Noe that 'Perceptron Convergence algorrithm' is also a variant of the stochastic gradient descent algorithm except it uses the stepwise activation function.
 
##### Linear Regression | Gradient descent | Stochastic GD | Ridge | [code](https://github.com/algostatml/SUPERVISED-ML/blob/master/REGRESSION/Regression.py)

##### Linear regression is a foremost statistical model for predictive learning. Here we solved the regression problem by deriving its closed form solution. Since, the closed form solution is computationally expensive especially in solving the matrix inverse with $O(N^{3})$, researchers developed a stochastic gradient descent algorithm for regression. 
